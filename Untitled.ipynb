{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 100 is out of bounds for axis 0 with size 100",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2a06abdaae56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m pi_b = FLMDP.scips_approximable_pi(lmdp=lmdp,\n\u001b[1;32m     39\u001b[0m                                    \u001b[0mGamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGamma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                                    sigma=1)\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m pi_e = FLMDP.random_pi(mag_S=mag_S,\n",
      "\u001b[0;32m~/code/lmdp/flmdp.py\u001b[0m in \u001b[0;36mscips_approximable_pi\u001b[0;34m(lmdp, Gamma, sigma, T, m)\u001b[0m\n\u001b[1;32m    216\u001b[0m                                           \u001b[0mrewards\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mr_t\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                                           \u001b[0ml\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlmdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m                                           Gamma=Gamma)\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhistory_action\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscips\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/lmdp/policy_approximators.py\u001b[0m in \u001b[0;36msparsity_corrected_approx\u001b[0;34m(states, actions, rewards, l, Gamma)\u001b[0m\n\u001b[1;32m    104\u001b[0m                     state_context, 0) + 1\n\u001b[1;32m    105\u001b[0m                 state_action_context: Tuple[State, Position, Action] = (\n\u001b[0;32m--> 106\u001b[0;31m                     state_in_history, k, actions[t][i])\n\u001b[0m\u001b[1;32m    107\u001b[0m                 \u001b[0mpositional_state_action_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate_action_context\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                     positional_state_action_counts.get(\n",
      "\u001b[0;31mIndexError\u001b[0m: index 100 is out of bounds for axis 0 with size 100"
     ]
    }
   ],
   "source": [
    "\"\"\"Generate Sample Trajectories\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from flmdp import FLMDP\n",
    "\n",
    "import flmdp\n",
    "import policy_approximators\n",
    "\n",
    "from importlib import reload\n",
    "reload(flmdp)\n",
    "reload(policy_approximators)\n",
    "\n",
    "# L-MDP params\n",
    "mag_S = 9\n",
    "mag_A = 4\n",
    "l = 4\n",
    "\n",
    "# Deterministic initial state distribution\n",
    "P0 = np.zeros((mag_S))\n",
    "P0[0] = 1.0\n",
    "\n",
    "P = FLMDP.random_P(mag_S=mag_S,\n",
    "                  mag_A=mag_A,\n",
    "                  l=l)\n",
    "\n",
    "lmdp = FLMDP(mag_S=mag_S,\n",
    "             mag_A=mag_A,\n",
    "             P=P,\n",
    "             P0=P0,\n",
    "             l=l)\n",
    "\n",
    "# Trajectory Params\n",
    "T = 20\n",
    "m = 100\n",
    "Gamma = 0.9\n",
    "\n",
    "pi_b = FLMDP.scips_approximable_pi(lmdp=lmdp,\n",
    "                                   Gamma=Gamma,\n",
    "                                   sigma=1)\n",
    "\n",
    "pi_e = FLMDP.random_pi(mag_S=mag_S,\n",
    "                       mag_A=mag_A,\n",
    "                       l=l)\n",
    "\n",
    "s_b, a_b, r_b = lmdp.simulate(pi=pi_b,\n",
    "                              T=T, \n",
    "                              m=m)\n",
    "s_e, a_e, r_e = lmdp.simulate(pi=pi_e,\n",
    "                              T=T, \n",
    "                              m=m)\n",
    "\n",
    "print(s_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create Policy Estimators\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from policy_approximators import naive_approx\n",
    "from policy_approximators import sparsity_corrected_approx\n",
    "\n",
    "s_df = pd.DataFrame(s_b)\n",
    "a_df = pd.DataFrame(a_b)\n",
    "r_df = pd.DataFrame(r_b)\n",
    "\n",
    "# Naive Monte-Carlo Policy Estimator\n",
    "hat_b = naive_approx(states=s_df,\n",
    "                     actions=a_df,\n",
    "                     rewards=r_df,\n",
    "                     l=l)\n",
    "\n",
    "# Sparsity Corrected Policy Estimator\n",
    "Gamma = 0.9\n",
    "tilde_b = sparsity_corrected_approx(states=s_df,\n",
    "                                    actions=a_df,\n",
    "                                    rewards=r_df,\n",
    "                                    l=l,\n",
    "                                    Gamma=Gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.4462512978052833\n",
      "-0.21621007254419836\n",
      "-8.649278300203472\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Evaluate Policy Estimators\"\"\"\n",
    "\n",
    "import step_is as step_is_module\n",
    "from importlib import reload\n",
    "reload(step_is_module)\n",
    "\n",
    "step_is = step_is_module.step_is\n",
    "\n",
    "gamma = 0.9\n",
    "\n",
    "rho_pi = step_is(pi_b=pi_b, \n",
    "                 pi_e=pi_e,\n",
    "                 state_samples=s_b,\n",
    "                 action_samples=a_b,\n",
    "                 reward_samples=r_b,\n",
    "                 l=l,\n",
    "                 gamma=gamma)\n",
    "\n",
    "rho_hat = step_is(pi_b=hat_b,\n",
    "                  pi_e=pi_e,\n",
    "                  state_samples=s_b,\n",
    "                  action_samples=a_b,\n",
    "                  reward_samples=r_b,\n",
    "                  l=l,\n",
    "                  gamma=gamma)\n",
    "\n",
    "rho_tilde = step_is(pi_b=tilde_b,\n",
    "                    pi_e=pi_e,\n",
    "                    state_samples=s_b,\n",
    "                    action_samples=a_b,\n",
    "                    reward_samples=r_b,\n",
    "                    l=l,\n",
    "                    gamma=gamma)    \n",
    "\n",
    "print(rho_pi)\n",
    "print(rho_hat)\n",
    "print(rho_tilde)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
